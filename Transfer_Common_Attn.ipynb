{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2fI1GZVwAiCt"},"outputs":[],"source":["import IPython\n","from google.colab import output\n","\n","display(IPython.display.Javascript('''\n"," function ClickConnect(){\n","   btn = document.querySelector(\"colab-connect-button\")\n","   if (btn != null){\n","     console.log(\"Click colab-connect-button\"); \n","     btn.click() \n","     }\n","   \n","   btn = document.getElementById('ok')\n","   if (btn != null){\n","     console.log(\"Click reconnect\"); \n","     btn.click() \n","     }\n","  }\n","  \n","setInterval(ClickConnect,60000)\n","'''))\n","\n","print(\"Done.\")"]},{"cell_type":"markdown","metadata":{"id":"wF1xcBMtqAac"},"source":["##Drive Mount"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJZt3K7BXRAX"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"Q3UARXrDqDdy"},"source":["##Libraries"]},{"cell_type":"markdown","metadata":{"id":"-rFwqC52qFZt"},"source":["####install"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mKa_U0mJXXHU"},"outputs":[],"source":["pip install keras-self-attention"]},{"cell_type":"markdown","metadata":{"id":"ENPRmevGqIJK"},"source":["####import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"boI39r6zXaF1"},"outputs":[],"source":["# For data processing\n","import numpy as np\n","import math\n","from math import sqrt\n","\n","# For data processing and manipulation\n","import pandas as pd\n","import csv\n","\n","# For date calculations\n","import datetime\n","import time\n","\n","# For ploting data\n","import IPython\n","import IPython.display\n","\n","import itertools\n","from itertools import cycle\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","# For checking path\n","import os , gc\n","from os import path\n","import csv\n","import json\n","\n","\n","from scipy.stats import hmean\n","\n","from sklearn.metrics import mean_squared_error\n","\n","#tensorflow libs\n","from tensorflow import keras\n","from tensorflow.keras.callbacks import EarlyStopping , Callback\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from keras import backend\n","\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.layers import Dense , LSTM ,Dropout , PReLU , RepeatVector ,TimeDistributed, Attention\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical , plot_model\n","from tensorflow.keras import regularizers, constraints, initializers, activations\n","#from keras.layers.recurrent import Recurrent, _time_distributed_dense\n","from tensorflow.keras.layers import SimpleRNN as Recurrent\n","#from tensorflow.compat.v1.keras.layers import RNN \n","\n","from tensorflow.keras.layers import InputSpec\n","\n","from keras_self_attention import SeqSelfAttention\n","from keras.layers.merge import concatenate\n","\n","\n","tf.get_logger().setLevel('ERROR')\n","mpl.rcParams['figure.figsize'] = (8, 6)\n","mpl.rcParams['axes.grid'] = False"]},{"cell_type":"markdown","metadata":{"id":"G91gwXkNqP-w"},"source":["####Font"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3UZPirg0WyQ"},"outputs":[],"source":["plt.rcParams['font.family'] = 'serif'\n","plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFHL3k7mZKjG"},"outputs":[],"source":["# The data from 2004 to 2017 are taken\n","date_after = pd.to_datetime(\"1/1/2004  00:00:00 AM\")\n","date_before = pd.to_datetime(\"1/1/2018  00:00:00 AM\")"]},{"cell_type":"markdown","metadata":{"id":"_TcWz0d-qSTF"},"source":["##Rootpath"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FY1aYo3SXf_t"},"outputs":[],"source":["ROOTPATH = '/content/gdrive/MyDrive/Projects/Crime forcasting'"]},{"cell_type":"markdown","metadata":{"id":"60oub2YMqX00"},"source":["##Data Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"ggt9EROl4dk_"},"source":["####Load dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kP9oC5CkUHl5"},"outputs":[],"source":["#load chicago data\n","PATH_PDA = ROOTPATH+\"/datasets/Chicago/chicago_data_sf_2004_2017_for_police_da.csv\"\n","PATH_DA = ROOTPATH+\"/datasets/Chicago/chicago_data_sf_2004_2017_for_da.csv\"\n","PATH_MAIN = ROOTPATH + \"/datasets/Chicago/Crimes_-_2001_to_Present.csv\"\n","PATH_WEATHER = ROOTPATH+\"/datasets/Chicago/weather_data_chaicago_2004_2017.csv\"\n","PATH_TEMP = ROOTPATH+\"/datasets/Chicago/temp_data_chicago_2004_2017.csv\"\n","SF_ATTN_COMMON = ROOTPATH+'/model_commonAttn_mae_swish/attn_LSTM_common_feature'\n","SF_ATTN_HISTORY = ROOTPATH+'/history_commonAttn_mae_swish/attn_LSTM_common_feature.json'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOjUZNNLVvTv"},"outputs":[],"source":["dataset = pd.read_csv(PATH_DA)\n","dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6QggtzSDWPw9"},"outputs":[],"source":["if(dataset['PdDistrict'].isnull().sum()):\n","    dataset = dataset.drop( index= dataset.loc[dataset['PdDistrict'].isnull()].index[0]) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IVOz8-dmWRwN"},"outputs":[],"source":["dataset = dataset.reset_index(drop= True)\n","dataset['datetime'] = dataset['Date']\n","dataset['PdDistrict'].isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pw7tSFQ0XIJV"},"outputs":[],"source":["num_police_dept = dataset['PdDistrict'].nunique()\n","print(\"Police departement district = \",num_police_dept)\n","Police_dept_name = dataset['PdDistrict'].drop_duplicates().sort_values(ascending = True)\n","print(Police_dept_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cN6qROpRXJah"},"outputs":[],"source":["# Dictionary for mapping id to name of the unique police department\n","Police_dept_id_name = {i:Police_dept_name.values[i] for i in range (num_police_dept)}\n","print(\"Police Departments ID:Name Dict-> \\n\",Police_dept_id_name)\n","\n","# Dictionary for mapping name of the unique police department to id\n","Police_dept_name_id = {key:value for (value,key) in Police_dept_id_name.items()}\n","print(\"Police Departments Name:ID Dict-> \\n\",Police_dept_name_id)\n","\n","# Update the new column of dataframe with the value of the list \n","dataset[\"PdDID\"] = dataset['PdDistrict']\n","print(\"PdDID column -> \\n\", dataset[\"PdDID\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t16_SH64XPVV"},"outputs":[],"source":["dataset.to_csv(PATH_PDA,index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ti9q1s_xbMDy"},"outputs":[],"source":["dataset1 = pd.read_csv(PATH_PDA)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMn4ZS9raWD7"},"outputs":[],"source":["def processCategory(df):\n","    for col in df.columns:\n","        col = str(col)\n","        if(col=='Category'):\n","            continue\n","        if col=='datetime':\n","            continue\n","        df.drop(col , 1 , inplace=True)\n","\n","    Category = np.array(df['Category'].unique() , str)\n","    Category.sort()\n","    Category = Category.tolist()\n","    df['Category'] = df['Category'].apply(lambda x:float(Category.index(x)))\n","    Y = pd.DataFrame(to_categorical(df['Category']))\n","    df = df.join(Y , on=Y.index)\n","    df['count'] = df['Category'].apply(lambda x:float(1))\n","    df.drop('Category' , 1, inplace=True)\n","\n","\n","    df.set_index(\"datetime\" , inplace = True)\n","    df.index =  pd.to_datetime(df.index)\n","    return df, Category"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1D2m8Oxa5i7"},"outputs":[],"source":["df2 = processCategory(dataset1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhcP7cDkb98x"},"outputs":[],"source":["df2 , Category2 = df2\n","df2.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sRenBX3bS0b"},"outputs":[],"source":["weather_dataset = pd.read_csv(PATH_WEATHER)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzsDiZ2za58q"},"outputs":[],"source":["temperature_dataset = pd.DataFrame(weather_dataset[[\"valid_date_time\", \"temp\"]].copy())\n","temperature_dataset = temperature_dataset.rename(columns={\"valid_date_time\": \"datetime\",\"temp\" : \"temperature\"})\n","temperature_dataset[\"datetime\"] = pd.to_datetime(temperature_dataset[\"datetime\"])\n","temperature_dataset = temperature_dataset.dropna()\n","temperature_dataset = temperature_dataset[(temperature_dataset.datetime>=date_after) & (temperature_dataset.datetime<date_before)]\n","temperature_dataset = temperature_dataset.drop_duplicates(subset =[\"datetime\"])\n","temperature_dataset.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZoRtGL-cwuJ"},"outputs":[],"source":["temperature_dataset[\"temperature\"].isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMM2xkrwcyTZ"},"outputs":[],"source":["temperature_dataset.to_csv(PATH_TEMP,index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSuqjxbVSXJe"},"outputs":[],"source":["def processData(df,sample):\n","    '''\n","    Change the sorted crime category with their index value\n","    & Resample Date time using hour\n","    input:\n","        df  : Dataset to process\n","        sample  : How to resample dataset df\n","    output:\n","        df_sample : resampled dataset df\n","    '''\n","\n","    df_sample = df.resample(sample).sum()\n","    return df_sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CLn4SZPPdCtR"},"outputs":[],"source":["df2 = processData(df2 , '1H')\n","df2.index = pd.to_datetime(df2.index)\n","df2 = pd.merge(df2 , temperature_dataset , on='datetime')\n","df2.set_index('datetime' , inplace=True)\n","df2.index = pd.to_datetime(df2.index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqP7_BbOdMBi"},"outputs":[],"source":["df2.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUKASAOgdRXx"},"outputs":[],"source":["def dateTimeToSignal(df):\n","    '''\n","    Converts the DateTime index to timestamp and convert it to signal (sine and cosine) to deal with periodicity\n","    input: \n","        df : Dataset\n","    Output:\n","        df : Dataset with column Day sin , Day cos , Week sin , Week cos ,  Year sin , Year cos ; representing Sin / Cosine signal for timestamp  \n","    '''\n","    timestamp_s = df.index.map(datetime.datetime.timestamp)\n","    day = 24*60*60\n","    week = 7*day\n","    year = (365.2425)*day\n","\n","    df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n","    df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n","\n","    df['Week sin'] = np.sin(timestamp_s * (2 * np.pi / week))\n","    df['Week cos'] = np.cos(timestamp_s * (2 * np.pi / week))\n","\n","    df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n","    df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYsmtQZidkcB"},"outputs":[],"source":["df2 = dateTimeToSignal(df2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9e66gwceTzA"},"outputs":[],"source":["df2.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zaH4i1YqeruS"},"outputs":[],"source":["# Group define\n","GROUPS = [[15],\n","     [2],\n","     [5],\n","     [8],\n","     [1],\n","     [21],\n","     [3],\n","     [29,7],\n","     [26,6],\n","     [30,22,24,19,27,12,9,16,0,10,14,13,28,18,17,4,23,20,11,25]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ArTybJReuAw"},"outputs":[],"source":["def make_groups(df):\n","    '''\n","    Make 10 groups from 38 different type of crime categories. \n","    Where,\n","      GR0 consits of,\n","        29 \tVEHICLE THEFT \n","        7 \tDECEPTIVE PRACTICE\n","      GR1 consits of,\n","        26 \tROBBERY \n","        6 \tCRIMINAL TRESPASS\n","      GR2 consits of,\n","        30 \tWEAPONS VIOLATION \t\t\n","        22 \tPROSTITUTION \t\t\t\t\n","        24 \tPUBLIC PEACE VIOLATION \t\t\n","        19 \tOFFENSE INVOLVING CHILDREN \t\n","        27 \tSEX OFFENSE \t\t\t\n","        12 \tINTERFERENCE WITH PUBLIC OFFICER \t\n","        9 \tGAMBLING \t\t\t\t\n","        16 \tLIQUOR LAW VIOLATION \t\t\t\n","        0 \tARSON \t\t\t\t  \t\n","        10 \tHOMICIDE \t\t\t\t\n","        14 \tKIDNAPPING \t\t\t\t\n","        13 \tINTIMIDATION \t\t\t\t\n","        28 \tSTALKING \t\t\t\t\n","        18 \tOBSCENITY \t\t\t\t \n","        17 \tNON-CRIMINAL \t\t\t\t \n","        4 \tCONCEALED CARRY LICENSE VIOLATION \t \n","        23 \tPUBLIC INDECENCY \t\t\t \n","        20 \tOTHER NARCOTIC VIOLATION \t\t  \n","        11 \tHUMAN TRAFFICKING \t\t\t  \n","        25 \tRITUALISM \n","\n","    input:\n","        df : Dataset with count of individual's from 38 categories\n","    output:\n","        df : Dataset with count of 10 groups  \n","    '''\n","    cnt = 0\n","    for i in GROUPS:\n","        cols_to_sum = i\n","        if(len(i)==1):\n","            continue\n","        else:\n","            newname = \"GRP\"+str(cnt)\n","            cnt = cnt + 1\n","            df[newname] = df[cols_to_sum].sum(axis=1)\n","            for j in i:\n","                df.drop(j , 1 , inplace=True)\n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLvpJ6n0exHR"},"outputs":[],"source":["df2 = make_groups(df2) \n","df2 = df2.rename(columns={15:1,2: 2, 5 : 3,8 : 4,1 : 5,21 : 6,3 : 7})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gx01j_IOe6O4"},"outputs":[],"source":["def temp_scale(x):\n","    '''\n","    x <= 273K or 0C -> low(0) \n","    x > 303K or 30C-> high(2)\n","    else medium(1)\n","    '''\n","    if(x<=0):\n","        return 0\n","    elif(x>30):\n","        return 2\n","    else:\n","        return 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gtfyfWKSe8LI"},"outputs":[],"source":["def split_data(df):\n","    '''\n","    split the dataset ( train , val , test = (70 , 20 , 10)% )\n","    input:\n","        df : Dataset to split\n","    output:\n","        train_df : train dataset\n","        val_df : validation dataset\n","        test_df : test dataset\n","        num_features : Number of features in dataset\n","        column_indices : column_indices in dataset\n","    '''\n","    column_indices = {name: i for i, name in enumerate(df.columns)}\n","\n","    n = len(df)\n","    train_df = df[0:int(n*0.7)]\n","    val_df = df[int(n*0.7):int(n*0.9)]\n","    test_df = df[int(n*0.9):]\n","\n","    num_features = df.shape[1]\n","    return train_df , val_df , test_df , num_features , column_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxzJUl0XfDEY"},"outputs":[],"source":["class WindowGenerator():\n","    '''\n","    WindowGenerator Class\n","    1. Split windows of features into a (features, labels) pairs.\n","    2. Plot the content of the resulting windows.\n","    3. Efficiently generate batches of these windows from the training, evaluation, and test data, using tf.data.Datasets S.\n","    '''\n","    def __init__(self, input_width, label_width, shift,\n","               train_df, val_df, test_df,\n","               label_columns=None , shuffle=False , batch_size = 64):\n","        '''\n","        The __init__ method includes all the necessary logic for the input and label indices.\n","        Input: \n","            input_width : input width / window size \n","            label_width : output width\n","            shift : size of window shifting forward\n","            train_df : train dataset\n","            val_df : validation dataset\n","            test_df : test dataset\n","            label_columns ( Default = None) : Label Columns\n","            shuffle ( Default = False) : weather to shuffle data \n","            batch_size (Default = 64) : Batch Size\n","        Output: None\n","        Example :\n","            w2 = WindowGenerator(input_width=6, label_width=1, shift=1,\n","                     label_columns=['count'])\n","            w2\n","\n","        '''\n","        # Store the raw data.\n","        self.train_df = train_df\n","        self.val_df = val_df\n","        self.test_df = test_df\n","        self.shuffle = shuffle\n","        self.batch_size = batch_size\n","\n","        # Work out the label column indices.\n","        self.label_columns = label_columns\n","        if label_columns is not None:\n","            self.label_columns_indices = {name: i for i, name in\n","                                        enumerate(label_columns)}\n","            \n","        self.column_indices = {name: i for i, name in\n","                            enumerate(train_df.columns)}\n","\n","\n","        # Work out the window parameters.\n","        self.input_width = input_width\n","        self.label_width = label_width\n","        self.shift = shift\n","\n","        self.total_window_size = input_width + shift\n","\n","        self.input_slice = slice(0, input_width) #(start , stop)\n","        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n","\n","        self.label_start = self.total_window_size - self.label_width\n","        self.labels_slice = slice(self.label_start, None)\n","        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n","\n","    def __repr__(self):\n","        return '\\n'.join([\n","            f'Total window size: {self.total_window_size}',\n","            f'Input indices: {self.input_indices}',\n","            f'Label indices: {self.label_indices}',\n","            f'Label column name(s): {self.label_columns}'])\n","        \n","    def split_window(self, features):\n","        '''\n","        Given a list consecutive inputs, the split_window method will convert them to a window of inputs and a window of labels.\n","        \n","        Input: \n","            features : Stack of array of datas , used for splitting data to inputs and labels\n","        Output:\n","            inputs : nd Array splitted as input_width\n","            labesl : nd Array splitted as label_width\n","        Example :\n","        # Stack three slices, the length of the total window:\n","            example_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n","                           np.array(train_df[100:100+w2.total_window_size]),\n","                           np.array(train_df[200:200+w2.total_window_size])])\n","\n","\n","            example_inputs, example_labels = w2.split_window(example_window)\n","\n","            print('All shapes are: (batch, time, features)')\n","            print(f'Window shape: {example_window.shape}')\n","            print(f'Inputs shape: {example_inputs.shape}')\n","            print(f'labels shape: {example_labels.shape}')\n","        '''\n","        inputs = features[:, self.input_slice, :]\n","        labels = features[:, self.labels_slice, :]\n","        #taking only the labels that are presentin the label_columns\n","        if self.label_columns is not None:\n","            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns],axis=-1)\n","\n","        # Slicing doesn't preserve static shape information, so set the shapes\n","        # manually. This way the `tf.data.Datasets` are easier to inspect.\n","        inputs.set_shape([None, self.input_width, None])\n","        labels.set_shape([None, self.label_width, None])\n","\n","        return inputs, labels\n","\n","    def plot(self, model=None, plot_col='count', max_subplots=3):\n","        '''\n","            plot method that allows a simple visualization of the split window,\n","            Input:\n","                model (Default=None) : tensorflow model to evaluate \n","                plot_col ( Default = 'count') : Name of column to evaluate\n","                max_subplots ( Default = 3) : Maximum Number of subplotting\n","            Output:\n","                None\n","            Example:\n","                w2.plot()\n","                w2.plot(plot_col=0) # label wont be shown as w2 config has only column , count \n","\n","        '''\n","        inputs, labels = self.example\n","        plt.figure(figsize=(12, 8))\n","        plot_col_index = self.column_indices[plot_col]\n","        max_n = min(max_subplots, len(inputs))\n","        for n in range(max_n):\n","            plt.subplot(max_n, 1, n+1)\n","            plt.ylabel(f'{plot_col} [normed]')\n","            plt.plot(self.input_indices, inputs[n, :, plot_col_index],label='Inputs', marker='.', zorder=-10)\n","\n","            if self.label_columns:\n","                label_col_index = self.label_columns_indices.get(plot_col, None)\n","            else:\n","                label_col_index = plot_col_index\n","\n","            if label_col_index is None:\n","                continue\n","\n","            plt.scatter(self.label_indices, labels[n, :, label_col_index],edgecolors='k', label='Labels', c='#2ca02c', s=64)\n","            if model is not None:\n","                predictions = model(inputs)\n","                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n","                        marker='X', edgecolors='k', label='Predictions',c='#ff7f0e', s=64)\n","            if n == 0:\n","                plt.legend()\n","        plt.xlabel('Time [h]')\n","\n","    def make_dataset(self, data):\n","        '''\n","        make_dataset method will take a time series DataFrame and convert it to a\n","            tf.data.Dataset of (input_window, label_window) pairs using the preprocessing.timeseries_dataset_from_array function.\n","        Input:\n","            data :  Input data to transform into (input_window , label_window)\n","        Output:\n","            ds : transformed dataset\n","        '''\n","        data = np.array(data, dtype=np.float32)\n","        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n","            data=data,\n","            targets=None,\n","            sequence_length=self.total_window_size,\n","            sequence_stride=1,\n","            shuffle=self.shuffle,\n","            batch_size=self.batch_size,)\n","        ds = ds.map(self.split_window)\n","        return ds\n","    \n","    \n","    def create_dataset2(self , map_df , reshape=True):\n","      x = []\n","      y = []\n","      for res in iter(map_df):\n","        inputs, labels = res\n","        if(len(inputs)==64):\n","          x.append(inputs)\n","          y.append(labels)\n","      \n","      x = np.array(x)\n","      y = np.array(y)\n","      if(reshape):\n","        x = x.reshape(-1, x.shape[-2] , x.shape[-1])\n","        y = y.reshape(-1 , y.shape[-2] , y.shape[-1])\n","      return x , y\n","    \n","    '''\n","    properties for accessing  training, validation and test data as tf.data.Datasets using the above make_dataset method.\n","    Also a standard example batch for easy access and plotting\n","    '''\n","\n","    @property\n","    def train(self):\n","        return self.make_dataset(self.train_df)\n","\n","    @property\n","    def val(self):\n","        return self.make_dataset(self.val_df)\n","\n","    @property\n","    def test(self):\n","        return self.make_dataset(self.test_df)\n","    \n","    \n","    @property\n","    def example(self):\n","        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n","        result = getattr(self, '_example', None)\n","        if result is None:\n","            # No example batch was found, so get one from the `.train` dataset\n","            result = next(iter(self.train))\n","            # And cache it for next time\n","            self._example = result\n","        return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mRSIMLvsfFwh"},"outputs":[],"source":["def create_data(train , test , val , columns):\n","    '''\n","    Create dataset from main train , test , val with given columns\n","    '''\n","    if(columns==None):\n","        columns = train.columns\n","    new_train = train[columns]\n","    new_test = test[columns]\n","    new_val = val[columns]\n","    return new_train , new_test , new_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yVSJQ87fHDY"},"outputs":[],"source":["def save_history(history , path):\n","    # convert the history.history dict to a pandas DataFrame:     \n","    hist_df = pd.DataFrame(history.history) \n","    # or save to csv: \n","    hist_csv_file = path\n","    try:\n","      with open(hist_csv_file, mode='w') as f:\n","        hist_df.to_json(f)\n","    except:\n","      return None\n","\n","def get_history(path):\n","\twith open(path) as json_file:\n","\t\tdata = json.load(json_file)\n","\t\treturn data\n","\n","def save_model_weights(model , path):\n","  model.save_weights(path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74Jq0i9gIeF4"},"outputs":[],"source":["def R_squared(y, y_pred):\n","  residual = tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n","  total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n","  r2 = tf.subtract(1.0, tf.divide(residual, total))\n","  return r2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvLS4Qbz0-RC"},"outputs":[],"source":["def sMAPE(y, y_pred):\n","    epsilon = 0.1\n","    summ = backend.maximum(backend.abs(y) + backend.abs(y_pred) + epsilon, 0.5 + epsilon)\n","    smape = backend.abs(y_pred - y) / summ * 2.0\n","    return smape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2EsFv89fw5B"},"outputs":[],"source":["plt.rcParams.update({'font.size': 30})\n","metrics_name = [\"MAE\" , \"MSLE\" , \"MSE\" , \"SMAPE\" , \"R^2\"]\n","def compileModel(model):\n","    model.compile(loss=tf.losses.MeanAbsoluteError(),optimizer=tf.optimizers.Adam(),\n","                  metrics=[tf.metrics.MeanSquaredLogarithmicError() , tf.metrics.MeanSquaredError() , sMAPE , R_squared ])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oSlRpml2fInQ"},"outputs":[],"source":["MAX_EPOCHS = 200\n","\n","def compile_and_fit(model, window, name=None , patience=10):\n","    '''\n","    Compile and fit a model\n","    '''\n","    \n","    if(name!=None):\n","        model_path = ROOTPATH+'/model_chicago_common_mae_swish/'+name\n","        history_path = ROOTPATH+'/history_chicago_common_mae_swish/'+name+\".json\"\n","    \n","        #plot_model(model, model_plot_path, show_shapes=True , expand_nested=True)\n","\n","    if not path.exists(ROOTPATH+'/model_chicago_common_mae_swish/'):\n","        os.makedirs(ROOTPATH+'/model_chicago_common_mae_swish/')\n","        os.makedirs(ROOTPATH+'/history_chicago_common_mae_swish/')\n","\n","    if(name!=None and path.exists(model_path)):\n","        print(\"Loaded Pre Trained Model\")\n","        modelOld = tf.keras.models.load_model(model_path , custom_objects={\"sMAPE\":sMAPE , \"R_squared\":R_squared})\n","        model.set_weights(modelOld.get_weights())\n","        history = get_history(history_path)\n","        return model , history\n","\n","    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)\n","    \n","    history = model.fit(window.train, epochs=MAX_EPOCHS,validation_data=window.val,callbacks=[early_stopping])\n","    if(name!=None):\n","        model.save(model_path)\n","        save_history(history , history_path)\n","    return model , history.history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_x_JS-OfQb4"},"outputs":[],"source":["general_indexs2 = [ 1,2,3,4,5,6,7,'count', 'Day sin' , 'Day cos' , 'Week cos' , 'Week sin', 'Year sin' , 'Year cos' , 'GRP0','GRP1','GRP2']\n","x_col2 = ['Day sin' , 'Day cos' , 'Year sin' , 'Year cos' , 'Week cos' , 'Week sin' , 'datetime']\n","def generate_window(df_now):\n","    train_df , val_df , test_df , num_features_df , column_indices_df = split_data(df_now)\n","    train_df , test_df , val_df = create_data(train_df , test_df , val_df , general_indexs2)\n","    \n","    y_col = []\n","    for i in train_df.columns:\n","        if(i in x_col2):\n","            continue\n","        y_col.append(i)\n","    wide_window_all = WindowGenerator(train_df=train_df, test_df=test_df , val_df=val_df, input_width=24, label_width=24, shift=1, label_columns=y_col)\n","    \n","    return wide_window_all    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B4mRF99pfUQB"},"outputs":[],"source":["input_attn_lstm = keras.Input(shape=(24,17), name=\"input_attn\")\n","x_attn = (keras.layers.Bidirectional(tf.keras.layers.LSTM(128, activation = tf.keras.activations.swish ,return_sequences=True, name= \"bdLSTMAttn\")))(input_attn_lstm)\n","x_attn = (SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation='swish', name='Attention'))(x_attn)\n","# x_attn = (keras.layers.Dense(units=64, activation = tf.keras.activations.swish, name=\"TimeDisDense1\"))(x_attn)\n","x_attn = (TimeDistributed(keras.layers.Dense(units=64, activation = tf.keras.activations.swish), name=\"TimeDisDense2\"))(x_attn)\n","output_attn_lstm = (keras.layers.Dense(units=11))(x_attn)\n"," \n","attn_lstm_model_common_feature = keras.Model(\n","    inputs=[input_attn_lstm],\n","    outputs=[output_attn_lstm],\n",")\n","attn_lstm_model_common_feature.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1aYWNlQS1IF"},"outputs":[],"source":["val_performance_attn = {}\n","performance_attn = {}\n","MAX_EPOCHS = 200\n","wide_window2 = generate_window(df2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaRsVo93ficX"},"outputs":[],"source":["attn_lstm_model_common_feature = compileModel(attn_lstm_model_common_feature)\n","\n","attn_lstm_model_common_feature , history_bd  = compile_and_fit(attn_lstm_model_common_feature, window=wide_window2 , name=\"attn_common_no_transfer\")\n","IPython.display.clear_output()\n","print(\"Model is Loaded!!\")\n","\n","val_performance_attn['attn_LSTM_common_feature'] = attn_lstm_model_common_feature.evaluate(wide_window2.val)\n","performance_attn['attn_LSTM_common_feature'] = attn_lstm_model_common_feature.evaluate(wide_window2.test)\n","# IPython.display.clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1klID_Pi4mV"},"outputs":[],"source":["print(len(history_bd))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XnjUh9DEgoB8"},"outputs":[],"source":["def transfer_model_load():\n","    \n","    if(path.exists(SF_ATTN_COMMON)):\n","        print(\"Loaded Pre Trained Model\")\n","        model = tf.keras.models.load_model(SF_ATTN_COMMON)\n","        history = get_history(SF_ATTN_HISTORY)\n","        return model , history\n","    else:\n","        print(\"Not found : \"+SF_ATTN_COMMON)\n","        return None , None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDFPXkHfiUMM"},"outputs":[],"source":["transfer_model, trasnfer_history = transfer_model_load()\n","IPython.display.clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BR0VHXeoiVsw"},"outputs":[],"source":["transfer_model = compileModel(transfer_model)\n","transfer_model , history  = compile_and_fit(transfer_model, wide_window2 , 'attn_common_transfer')\n","\n","val_performance_attn['chicago_attn_common_transfer'] = transfer_model.evaluate(wide_window2.val)\n","performance_attn['chicago_attn_common_transfer'] = transfer_model.evaluate(wide_window2.test)\n","\n","# IPython.display.clear_output()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Transfer_Common_Attn.ipynb","provenance":[{"file_id":"1DWpRkedOxm06WvXMxJnqychMD0nt80eq","timestamp":1647694877104}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
